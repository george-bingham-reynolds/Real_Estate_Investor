{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc2e6cf-9d5c-447a-82d5-5153a9353276",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math as m\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import os\n",
    "from sklearn import linear_model as lm, metrics, ensemble as ens\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bea657-5399-432e-82a0-e4c29ef09717",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"full_info.csv\")\n",
    "df['state'] = df['state'].apply(lambda x: re.sub('District Of Columbia', 'District of Columbia', x))\n",
    "df = df[df['total_population'] >= 50000]\n",
    "\n",
    "df_latest = pd.read_csv(\"latest_years.csv\")\n",
    "df_latest = df_latest.drop(columns = ['rent_in_three', 'three_year_growth'])\n",
    "df_latest = df_latest[df_latest['total_population'] >= 50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240b1ab1-4b52-4ce3-ae2c-abb10b31d1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, prefix = 'state', columns = ['state'])\n",
    "df_latest = pd.get_dummies(df_latest, prefix = 'state', columns = ['state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b282946-3e7c-4dda-b1b5-d89d46d94bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    if 'state' in col:\n",
    "        df[col] = df[col].apply(lambda x: 1 if x == True else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5f3491-6516-48f2-9f2e-0df16a26f2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_latest = df_latest[df_latest['year'] == df_latest['year'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880d384e-ca9e-4127-ab0e-f293fee80e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mean current value: ', df_latest['average_home_value'].mean())\n",
    "print('median current value: ', df_latest['average_home_value'].median())\n",
    "print('25th percentile current value: ', np.percentile(df_latest['average_home_value'], 25))\n",
    "print('number 200k or less: ', len(df_latest[df_latest['average_home_value'] <= 200000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b805c7-94f2-4f9f-ae5a-8050679faa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_latest_candidates = df_latest.sort_values(by = 'average_home_value').drop(columns = [x for x in df_latest.columns if x != 'place' and\\\n",
    "                                        x != 'year' and x != 'average_annual_rent' and\\\n",
    "                                        x != 'average_home_value' and x != 'total_population' and \\\n",
    "                                                                                        # '3' not in x and\\\n",
    "                                        'roi' not in x and 'vacancy' not in x])\n",
    "\n",
    "df_latest_candidates = df_latest_candidates[~df_latest_candidates['place'].str.contains('California')]\n",
    "df_latest_candidates = df_latest_candidates[~df_latest_candidates['place'].str.contains('New York')]\n",
    "df_latest_candidates = df_latest_candidates[~df_latest_candidates['place'].str.contains('Puerto Rico')]\n",
    "df_latest_candidates = df_latest_candidates[~df_latest_candidates['place'].str.contains('Hawaii')]\n",
    "df_latest_candidates = df_latest_candidates[~df_latest_candidates['place'].str.contains('Texas')]\n",
    "df_latest_candidates = df_latest_candidates[~df_latest_candidates['place'].str.contains('Florida')]\n",
    "df_latest_candidates = df_latest_candidates[df_latest_candidates['average_home_value'] <= 250000]\n",
    "df_latest_candidates = df_latest_candidates[df_latest_candidates['total_population'] >= 100000]\n",
    "df_latest_candidates = df_latest_candidates[df_latest_candidates['vacancy_growth_last_1_years'] < 0]\n",
    "df_latest_candidates = df_latest_candidates[df_latest_candidates['vacancy_growth_last_3_years'] < df_latest_candidates['vacancy_growth_last_2_years']]\n",
    "df_latest_candidates = df_latest_candidates[df_latest_candidates['vacancy_growth_last_2_years'] < df_latest_candidates['vacancy_growth_last_1_years']] \n",
    "df_latest_candidates.sort_values(by = 'vacancy_rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6975626-3e1d-4aed-ad12-ff3d53b8ff17",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_latest['average_home_value'], bins = 35)\n",
    "plt.title(\"Average Home Value Distribution\")\n",
    "plt.xlabel(\"Value (in millions)\")\n",
    "plt.ylabel(\"Number of Counties\")\n",
    "plt.savefig(\"Home_Values_Dist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e47b2d-5fc9-4ad5-81cb-f43d5323aa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_latest['vacancy_rate'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc9efba-b8c5-4bcd-8e61-c79ea741dee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_latest['vacancy_rate'], bins = 50)\n",
    "plt.axvline(df_latest['vacancy_rate'].mean(), color = 'r', linestyle = 'dashed', label = 'Mean Vacancy Rate')\n",
    "plt.axvline(df_latest_candidates['vacancy_rate'].mean(), color = 'k', linestyle = 'dashed', label = 'Candidate Mean Vacancy Rate')\n",
    "plt.legend()\n",
    "plt.title(\"Vacancy Rate Distribution\")\n",
    "plt.xlabel(\"Vacancy Rate\")\n",
    "plt.ylabel(\"Number of Counties\")\n",
    "plt.savefig(\"Vacancy_Dist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583f079d-f137-4617-9842-fd0326997e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mean current vacancy rate: ', df_latest['vacancy_rate'].mean())\n",
    "print('median current vacancy rate: ', df_latest['vacancy_rate'].median())\n",
    "print('25th percentile current vacancy rate: ', np.percentile(df_latest['vacancy_rate'], 25))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eca353a-bd79-4c93-a751-b5ca9b50c4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_latest[df_latest['vacancy_rate'] < 0.2]['vacancy_rate'], bins = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14f8095-c3b6-494a-ad0a-87dd26b5e544",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281f0ee5-14b3-4486-a971-703a5b2c1bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE VALI DATA SECOND TO LAST YEAR; TEST DATA LAST YEAR; TRAIN FIRST 6\n",
    "last_year = df['year'].max()\n",
    "train = df[df['year'] < last_year - 4]\n",
    "vali = df[df['year'] == last_year - 1]\n",
    "test = df[df['year'] == last_year]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c559185-813a-4b5c-8065-d3a5c3dedd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('training sample - ', len(train))\n",
    "print('validation sample - ', len(vali))\n",
    "print('test sample - ', len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0e7ce1-b655-4df4-afe6-7d8cbb157236",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for yr in set(df['year'].values.tolist()):\n",
    "    print(yr, len(df[df['year'] == yr]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28164dc7-e065-449f-9d6d-b78dbe8233a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(df))\n",
    "print(len(df_latest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5728aa71-9bed-4666-af63-61cd9c393935",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TESTING HAS SHOWN FAKE PREDS BEATS MODEL - WANNA SEE WHAT HAPPENS IF FAKE PRED IS A FEATURE\n",
    "train['fake_pred'] = round(((train['average_home_value'] * (1 + train['home_val_growth_last_3_years']) - train['average_home_value']) + \\\n",
    "                     (\n",
    "                        train['average_annual_rent'] * (1 + train['rent_growth_last_1_years']) +\\\n",
    "                        train['average_annual_rent'] * (1 + train['rent_growth_last_2_years']) +\\\n",
    "                        train['average_annual_rent'] * (1 + train['rent_growth_last_3_years'])\n",
    "                     ))/train['average_home_value'], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ba3068-1b5a-4cf6-a6ad-aaaf6c4da6b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vali['fake_pred'] = round(((vali['average_home_value'] * (1 + vali['home_val_growth_last_3_years']) - vali['average_home_value']) + \\\n",
    "                     (\n",
    "                        vali['average_annual_rent'] * (1 + vali['rent_growth_last_1_years']) +\\\n",
    "                        vali['average_annual_rent'] * (1 + vali['rent_growth_last_2_years']) +\\\n",
    "                        vali['average_annual_rent'] * (1 + vali['rent_growth_last_3_years'])\n",
    "                     ))/vali['average_home_value'], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d7d251-1365-461c-b6f6-bd6da373af15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X, Y SPLITS\n",
    "std_scl = StandardScaler()\n",
    "\n",
    "ex_train = train.drop(columns = ['year', 'place', 'roi'])\n",
    "ex_train_scaled = std_scl.fit_transform(ex_train)\n",
    "why_train = train['roi']\n",
    "\n",
    "\n",
    "ex_vali = vali.drop(columns = ['year', 'place', 'roi'])\n",
    "ex_vali_scaled = std_scl.fit_transform(ex_vali)\n",
    "why_vali = vali['roi']\n",
    "\n",
    "ex_test = test.drop(columns = ['year', 'place', 'roi'])\n",
    "ex_test_scaled = std_scl.fit_transform(ex_test)\n",
    "why_test = test['roi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f36113b-4b53-4248-b63d-3d8c9a5b3a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = metrics.mean_squared_error\n",
    "mae = metrics.mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cf18ab-576d-43ca-8adf-edba9100d2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('actuals mean - ', why_vali.mean())\n",
    "print('actuals standard dev - ', why_vali.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7c0853-6d9c-4f03-9ca1-7d860f41a4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('rmse - ', m.sqrt(mse(why_vali, vali['fake_pred'])))\n",
    "print('mae - ', mae(why_vali, vali['fake_pred']))\n",
    "vali = vali.drop(columns = ['fake_pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d7f9ad",
   "metadata": {},
   "source": [
    "QUICK NOTE THAT I USED TO INCLUDE LOGISTIC REGRESSION; RESULTS WERE POOR TO THE POINT OF NOT BEING WORTH FURTHER EXPLORATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3056b277",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = ens.RandomForestRegressor(random_state=123)\n",
    "\n",
    "# TRAIN/VALI\n",
    "rf_fit = rf.fit(ex_train, why_train)\n",
    "rf_preds = rf_fit.predict(ex_vali)\n",
    "print('rmse - ', m.sqrt(mse(why_vali, rf_preds)))\n",
    "print('mae - ', mae(why_vali, rf_preds))\n",
    "\n",
    "# PERFORMANCE BEEN BAD SO WANNA SHPEEP TRAINING SCORES TOO - ARE WE OVER OR UNDER FITTING?\n",
    "rf_train_preds = rf.fit(ex_train, why_train).predict(ex_train)\n",
    "print('training rmse - ', m.sqrt(mse(why_train, rf_train_preds)))\n",
    "print('training mae - ', mae(why_train, rf_train_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cd9f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ex_train.columns\n",
    "importances = rf_fit.feature_importances_\n",
    "data = {'feature_names': feature_names, 'feature_importance': importances}\n",
    "rf_df = pd.DataFrame(data)\n",
    "rf_df.sort_values(by = ['feature_importance'], ascending = False, inplace = True)\n",
    "rf_df = rf_df.head(20)\n",
    "\n",
    "sns.barplot(x = rf_df['feature_importance'], y = rf_df['feature_names'], ci = None)\n",
    "\n",
    "#ADD CHART LABELS\n",
    "plt.title('Random Forest Feature Importance')\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Feature Names')\n",
    "plt.savefig(\"feature_importance\", bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af13f8e-219c-4a19-b1a9-478e42d53c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r rf_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f78577-0ca4-437f-ae92-1cc750895306",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f19815-1ddc-470f-8e46-1dac81dbf2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if sum([1 if 'pca__' in x else 0 for x in rf_params]) > 0:\n",
    "    steps = [('pca', PCA(n_components = rf_params['pca__n_components'])), \n",
    "             ('rf', ens.RandomForestRegressor(n_estimators = rf_params['rf__n_estimators'], \n",
    "                                          max_depth = rf_params['rf__max_depth'],\n",
    "                                          criterion = rf_params['rf__criterion']))]\n",
    "    model_rf = Pipeline(steps = steps)\n",
    "\n",
    "    \n",
    "    \n",
    "else:\n",
    "    \n",
    "    model_rf = ens.RandomForestRegressor(n_estimators = rf_params['n_estimators'], \n",
    "                                          max_depth = rf_params['max_depth'],\n",
    "                                          criterion = rf_params['criterion'])\n",
    "    \n",
    "# TRAIN/VALI\n",
    "rf_fit = model_rf.fit(ex_train, why_train)\n",
    "rf_preds_tuned = rf_fit.predict(ex_vali)\n",
    "print('rmse - ', m.sqrt(mse(why_vali, rf_preds_tuned)))\n",
    "print('mae - ', mae(why_vali, rf_preds_tuned))\n",
    "\n",
    "# PERFORMANCE BEEN BAD SO WANNA SHPEEP TRAINING SCORES TOO - ARE WE OVER OR UNDER FITTING?\n",
    "rf_train_preds_tuned = model_rf.fit(ex_train, why_train).predict(ex_train)\n",
    "print('training rmse - ', m.sqrt(mse(why_train, rf_train_preds_tuned)))\n",
    "print('training mae - ', mae(why_train, rf_train_preds_tuned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8e8945-b2e8-4e6e-93cc-f74af370a4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r ada_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb87d0b6-bf89-4432-83e8-6806d4b31d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7377d8-f122-4e2a-9c1f-dea37cfc4777",
   "metadata": {},
   "outputs": [],
   "source": [
    "if sum([1 if 'pca__' in x else 0 for x in ada_params]) > 0:\n",
    "    steps = [('pca', PCA(n_components = ada_params['pca__n_components'])), \n",
    "             ('ada', ens.AdaBoostRegressor(n_estimators = ada_params['ada__n_estimators'], \n",
    "                                           learning_rate = ada_params['ada__learning_rate']))]\n",
    "             \n",
    "    ada = Pipeline(steps = steps)\n",
    "    \n",
    "else:\n",
    "    ada = ens.AdaBoostRegressor(n_estimators = ada_params['n_estimators'], \n",
    "                                learning_rate = ada_params['learning_rate'])\n",
    "\n",
    "    \n",
    "    \n",
    "# TRAIN/VALI\n",
    "ada_fit = ada.fit(ex_train, why_train)\n",
    "ada_preds = ada_fit.predict(ex_vali)\n",
    "print('rmse - ', m.sqrt(mse(why_vali, ada_preds)))\n",
    "print('mae - ', mae(why_vali, ada_preds))\n",
    "               \n",
    "# PERFORMANCE BEEN BAD SO WANNA SHPEEP TRAINING SCORES TOO - ARE WE OVER OR UNDER FITTING?\n",
    "ada_train_preds = ada.fit(ex_train, why_train).predict(ex_train)\n",
    "print('training rmse - ', m.sqrt(mse(why_train, ada_train_preds)))\n",
    "print('training mae - ', mae(why_train, ada_train_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6a8509-eadd-4847-8073-b674971ea038",
   "metadata": {},
   "outputs": [],
   "source": [
    "if sum([1 if 'pca__' in x else 0 for x in ada_params]) == 0:\n",
    "    feature_names = ex_train.columns\n",
    "    importances = ada_fit.feature_importances_\n",
    "    data = {'feature_names': feature_names, 'feature_importance': importances}\n",
    "    ada_df = pd.DataFrame(data)\n",
    "    ada_df.sort_values(by = ['feature_importance'], ascending = False, inplace = True)\n",
    "    ada_df = ada_df.head(20)\n",
    "\n",
    "    sns.barplot(x = ada_df['feature_importance'], y = ada_df['feature_names'], ci = None)\n",
    "\n",
    "    #ADD CHART LABELS\n",
    "    plt.title('AdaBoost Feature Importance')\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.ylabel('Feature Names')\n",
    "    # plt.savefig(\"adaboost_fi_diag_chrom_level\", bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b479b52-aef3-4a69-a3ef-1a332dcc4e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r xgboost_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20dd22e-1f47-495a-bdea-00fceaaf9918",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df570f56-a3c6-4d0b-b00e-3c7afa2ff25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBOOST IS BEING A CRYBABY ABOUT TYPES IDK WHY, BUT WHATEVER I'LL EXPLICITLY FIX LOL\n",
    "ex_train['average_annual_rent'] = ex_train['average_annual_rent'].astype(float)\n",
    "ex_train['average_home_value'] = ex_train['average_home_value'].astype(float)\n",
    "ex_train['average_income'] = ex_train['average_income'].astype(float)\n",
    "\n",
    "ex_vali['average_annual_rent'] = ex_vali['average_annual_rent'].astype(float)\n",
    "ex_vali['average_home_value'] = ex_vali['average_home_value'].astype(float)\n",
    "ex_vali['average_income'] = ex_vali['average_income'].astype(float)\n",
    "\n",
    "\n",
    "\n",
    "if sum([1 if 'pca__' in x else 0 for x in xgboost_params]) > 0:\n",
    "    steps = [('pca', PCA(n_components = xgboost_params['pca__n_components'])), \n",
    "             ('xg', XGBRegressor(scale_pos_weight = xgboost_params['xg__scale_pos_weight'],\n",
    "                                  max_depth = xgboost_params['xg__max_depth'], \n",
    "                                  eta = xgboost_params['xg__eta']))]\n",
    "             \n",
    "    xg = Pipeline(steps = steps)\n",
    "    \n",
    "else:\n",
    "    xg = XGBRegressor(scale_pos_weight = xgboost_params['scale_pos_weight'],\n",
    "                  max_depth = xgboost_params['max_depth'], \n",
    "                  eta = xgboost_params['eta'])\n",
    "\n",
    "# TRAIN/VALI\n",
    "xg_fit = xg.fit(ex_train, why_train)\n",
    "xg_preds = xg_fit.predict(ex_vali)\n",
    "print('rmse - ', m.sqrt(mse(why_vali, xg_preds)))\n",
    "print('mae - ', mae(why_vali, xg_preds))\n",
    "\n",
    "# PERFORMANCE BEEN BAD SO WANNA SHPEEP TRAINING SCORES TOO - ARE WE OVER OR UNDER FITTING?\n",
    "xg_train_preds = xg.fit(ex_train, why_train).predict(ex_train)\n",
    "print('training rmse - ', m.sqrt(mse(why_train, xg_train_preds)))\n",
    "print('training mae - ', mae(why_train, xg_train_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9f74dc-b671-4004-b7a8-6e11f84c0dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if sum([1 if 'pca__' in x else 0 for x in xgboost_params]) == 0:\n",
    "    feature_names = ex_train.columns\n",
    "    importances = xg_fit.feature_importances_\n",
    "    data = {'feature_names': feature_names, 'feature_importance': importances}\n",
    "    xg_df = pd.DataFrame(data)\n",
    "    xg_df.sort_values(by = ['feature_importance'], ascending = False, inplace = True)\n",
    "    xg_df = xg_df.head(20)\n",
    "\n",
    "    sns.barplot(x = xg_df['feature_importance'], y = xg_df['feature_names'], ci = None)\n",
    "\n",
    "    #ADD CHART LABELS\n",
    "    plt.title('XGBoost Feature Importance')\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.ylabel('Feature Names')\n",
    "    # plt.savefig(\"adaboost_fi_diag_chrom_level\", bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caa9e8a-c397-45c6-920a-fe9db85e9854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SO HEURISTIC MODEL IS ACUTALLY BEST = BASE PREDS ON THIS\n",
    "\n",
    "vali['predicted_roi'] = round(((vali['average_home_value'] * (1 + vali['home_val_growth_last_3_years']) - vali['average_home_value']) + \\\n",
    "                     (\n",
    "                        vali['average_annual_rent'] * (1 + vali['rent_growth_last_1_years']) +\\\n",
    "                        vali['average_annual_rent'] * (1 + vali['rent_growth_last_2_years']) +\\\n",
    "                        vali['average_annual_rent'] * (1 + vali['rent_growth_last_3_years'])\n",
    "                     ))/vali['average_home_value'], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dce8d3-4a0e-4368-b169-3a89424d2ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_check = vali.sort_values(by = 'predicted_roi', ascending = False)\n",
    "pred_check = pred_check.drop(columns = [x for x in df.columns if x != 'place' and\\\n",
    "                                        x != 'year' and x != 'average_annual_rent' and\\\n",
    "                                        x != 'average_home_value' and x != 'total_population' and\\\n",
    "                                        'roi' not in x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75c0237-fb33-4b82-bbcb-6ec73fd5faad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ec = vali.copy()\n",
    "df_ec['error'] = round(df_ec['predicted_roi'] - df_ec['roi'], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253d8ef4-f8e8-4b6d-8ddd-bb34094dfbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_ec['error'], bins = 30)\n",
    "plt.title(\"Residual Distribution \\nValidation Data\")\n",
    "plt.xlabel(\"Size of Error\")\n",
    "plt.ylabel(\"Number of Observations\")\n",
    "plt.savefig(\"Model_Residuals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3aec83",
   "metadata": {},
   "source": [
    "CHECKING IN, NONE OF THE MODELS (OFF THE SHELF OR TUNED) ACTUALLY BEAT THE \"DUMB\" BASELINE SET WITH A SIMPLE LINEAR COMBINATION OF THE LAST THREE YEARS OF GROWTH PROJECTED TO THE NEXT THREE. \n",
    "\n",
    "HAVING SAID THAT, THE BASELINE MODEL ACTUALLY PERFORMS WELL, BEATING STANDARD DEVIATION OF THE VALIDATION SET'S ROI. THIS IMPLIES THAT IT DOES BETTER THAN PREDICTING THE SET'S AVERAGE (THIS IS HARDER THAN IT SOUNDS AS WE DON'T HAVE THE AVERAGE AHEAD OF TIME), SO THE \"DUMB\" MODEL ITSELF IS ACTUALLY A FAIRLY USEFUL TOOL. THIS IS SEEMINGLY CONFIRMED BY THE ERROR'S (SOMEWHAT) NORMAL DISTRIBUTION ABOUT 0.\n",
    "\n",
    "NEXT, WE CHECK WITH A MORE \"REAL WORLD\" TEST. REALISTICALLY WE WOULD NOT BE BUYING AND SHORTING ON EVERY CITY'S REAL ESTATE MARKET, SO THE BETTER APPLICATION IS TO SEE WHAT WOULD HAVE HAPPENED IF WE HAD USED THE MODEL TO IDENTIFY TOP CANDIDATES IN THE LATEST YEAR OF TESTABLE DATA. I CONSIDER ANY CANDIDATE IDENTIFIED THAT WOULD HAVE PERFORMED BETTER THAN AVERAGE ACCEPTABLE (THRESHOLD IN CELL BELOW)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d2156d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred_check['roi'].mean())\n",
    "print(pred_check['roi'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401ef6e8-4b3f-44b5-a9a4-31804673b482",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_check.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bcc632",
   "metadata": {},
   "source": [
    "NO WOULD-BE CANDIDATES FALL BELOW OUR THRESHOLD, SO WHILE THE MODEL ISN'T PERFECT IT WOULD HAVE IDENTIFIED QUALITY CANDIDATES FROM WHICH TO CHOOSE (AKA IT'S USEFUL). \n",
    "\n",
    "HAVING SAID THAT, IT CONSISTENTLY OVERESTIMATED THESE CANDIDATES' PERFORMANCE. I NEXT CHECK MEAN PREDICTED ROI VERSUS MEAN ROI TO SEE IF THIS IS A WIDESPREAD PROBLEM. IN ADDITION, I CHECK THE TOP ACTUAL PERFORMERS TO SEE IF HOW THE MODEL WOULD HAVE RATED THEM (IT'S MORE IMPORTANT THAT IT'S HIGH RATINGS AREN'T ACTUALLY BAD, BUT IDEALLY ACTUALLY GOOD PERFORMERS WOULD NOT BE RATED AS BAD EITHER). FINALLY, I CHECK TO SEE HOW MUCH OVERLAP THERE WOULD HAVE BEEN BETWEEN TOP CANDIDATES IDENTIFIED AND TOP PERFORMERS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262143f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('predicted - ', pred_check['predicted_roi'].mean())\n",
    "print('actual - ', pred_check['roi'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f738868",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRETTY CLOSE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e11ce4-cbc5-42a3-b0d2-1f8f3d271d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_check.sort_values(by = 'roi', ascending = False).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11950e15-68c7-4c7d-9d4d-b52b389fe091",
   "metadata": {},
   "outputs": [],
   "source": [
    "[place for place in pred_check.head(25)['place'].values.tolist() if \\\n",
    " place in pred_check.sort_values(by = 'roi', ascending = False).head(25)['place'].values.tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5cadb3",
   "metadata": {},
   "source": [
    "SO, 2 OF THE TOP PERFORMERS HAD A BELOW AVERAGE PREDICTION, AND 5 CITIES WOULD HAVE BEEN TOP 25 IN BOTH PREDICTED AND ACTUAL ROI. GIVEN THAT THE TOP PREDICTED ALL WERE ABOVE AVERAGE, I'D CONCLUDE THAT WHILE THERE IS DEFINITELY ROOM FOR IMPROVEMENT, THE REAL ESTATE INVESTMENT MODEL WOULD HAVE DONE MEANINGFULLY BETTER THAN RANDOM.\n",
    "\n",
    "PUT ANOTHER WAY, WHILE I WOULD NOT RECOMMEND USING IT BLINDLY TO PICK A CITY FOR INVESTMENT, IT'S A USEFUL TOOL FOR NARROWING DOWN THE POOL OF CANDIDATES. WHILE YOU MAY MISS OUT ON THE TOP POSSIBLE INVESTMENT OPPORTUNITY, IT'S PREVIOUS PERFORMANCE SUGGESTS YOU'D BE LIKELY TO HAVE A SOLID PICK.\n",
    "\n",
    "FOR NEXT IMMEDIATE STEPS, I MAKE THE DATA AVAILABLE IN AN API. LONG-TERM, FURTHER RESEARCH ON A MACHINE-LEARNING-ENGINEERED SOLUTION TO BEAT OUR HEURISTIC WOULD BE IDEAL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5137b31f-064b-4cfc-a708-0eccaf811bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_latest = df_latest.dropna()\n",
    "df_latest = df_latest.drop(columns = [x for x in df_latest.columns if 'state_' in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb573a45-9676-4ac5-ae34-06ceacadef7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_latest['pred'] = round(((df_latest['average_home_value'] * (1 + df_latest['home_val_growth_last_3_years']) - df_latest['average_home_value']) + \\\n",
    "                     (\n",
    "                        df_latest['average_annual_rent'] * (1 + df_latest['rent_growth_last_1_years']) +\\\n",
    "                        df_latest['average_annual_rent'] * (1 + df_latest['rent_growth_last_2_years']) +\\\n",
    "                        df_latest['average_annual_rent'] * (1 + df_latest['rent_growth_last_3_years'])\n",
    "                     ))/df_latest['average_home_value'], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44472932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE PLACE EASIER FOR API I.E. NO SPACES; WANNA KEEP IN ORIGINAL DF IN CASE I WANT FURTHER ANALYSIS\n",
    "df_save = df_latest.copy()\n",
    "df_save['place'] = df_save['place'].apply(lambda x: re.sub(' ', '.', \n",
    "                                                           re.sub('St.', 'Saint', \n",
    "                                                                  re.sub(' - ', '.', x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e49a37c-ea87-4de8-9960-86d5cf6a424d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_save.to_csv(\"latest_yrs_w_preds_city.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ba4c31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
